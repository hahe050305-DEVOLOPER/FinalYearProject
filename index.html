<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>AuraCheck | Biometric Audit</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js" crossorigin="anonymous"></script>
    <style>
        body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; transition: background 0.8s cubic-bezier(0.4, 0, 0.2, 1); overflow: hidden; background: #f1f2f6; }
        #app { display: flex; flex-direction: column; align-items: center; padding: 15px; height: 100vh; justify-content: space-around; text-align: center; }
        .view-box { position: relative; width: 85vw; max-width: 320px; height: 320px; border-radius: 60px; overflow: hidden; border: 8px solid #fff; box-shadow: 0 20px 40px rgba(0,0,0,0.08); background: #000; }
        video { width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); }
        #emotion-label { position: absolute; top: 25px; width: 50%; left: 25%; padding: 12px; border-radius: 30px; background: rgba(255,255,255,0.95); font-weight: 900; font-size: 0.8rem; letter-spacing: 2px; z-index: 10; box-shadow: 0 4px 15px rgba(0,0,0,0.05); }
        #help-trigger { position: absolute; top: 20px; right: 20px; width: 35px; height: 35px; background: #fff; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; cursor: pointer; z-index: 11; box-shadow: 0 4px 10px rgba(0,0,0,0.1); }
        #advice { font-size: 1rem; font-weight: 500; padding: 0 30px; min-height: 80px; color: #1e272e; line-height: 1.5; max-width: 400px; }
        .status-bar { font-size: 0.65rem; color: #a1a1a1; font-weight: 700; letter-spacing: 1.5px; text-transform: uppercase; }
        #btn-capture { width: 85vw; max-width: 280px; padding: 20px; border-radius: 50px; border: none; background: #1e272e; color: white; font-weight: 700; cursor: pointer; letter-spacing: 1px; transition: transform 0.2s; }
        #btn-capture:active { transform: scale(0.96); }
        #modal-overlay { display: none; position: fixed; inset: 0; background: rgba(0,0,0,0.8); z-index: 1000; padding: 20px; align-items: center; justify-content: center; backdrop-filter: blur(5px); }
        .modal-content { background: #fff; color: #111; padding: 30px; border-radius: 35px; max-width: 350px; text-align: left; box-shadow: 0 25px 50px rgba(0,0,0,0.3); }
        .close-btn { background: #1e272e; color: white; padding: 12px; border-radius: 20px; border: none; font-weight: 700; width: 100%; margin-top: 20px; cursor: pointer; }
        .loading-overlay { position: fixed; inset: 0; background: #fff; display: flex; flex-direction: column; align-items: center; justify-content: center; z-index: 100; }
    </style>
</head>
<body>
    <div id="loading" class="loading-overlay">
        <div style="font-size: 3rem; margin-bottom: 15px;">ðŸ“¡</div>
        <p style="font-weight: 800; color: #1e272e; letter-spacing: 2px;">CALIBRATING SENSORS</p>
    </div>

    <div id="modal-overlay">
        <div class="modal-content">
            <h2 style="margin-top:0; font-size: 1.4rem;">Audit Triggers</h2>
            <ul style="padding-left: 20px; font-size: 0.9rem; line-height: 1.6; color: #444;">
                <li><strong>Thinking:</strong> Detected via head-axis deviation.</li>
                <li><strong>Frustration:</strong> Triggered by active corrugator muscle tension.</li>
                <li><strong>Sadness:</strong> Identified through inner-brow elevation or sustained gaze depression.</li>
                <li><strong>Organic Joy:</strong> Captured via cheek-lift elevation (>0.45).</li>
            </ul>
            <button class="close-btn" onclick="toggleModal()">PROCEED</button>
        </div>
    </div>

    <div id="app">
        <div class="view-box">
            <div id="help-trigger" onclick="toggleModal()">?</div>
            <div id="emotion-label">SYNCING</div>
            <video id="webcam" autoplay playsinline muted></video>
        </div>
        <div>
            <div id="advice">Awaiting biometric baseline...</div>
            <div id="status" class="status-bar">Aura Audit System v10.3</div>
        </div>
        <button id="btn-capture">GENERATE AURA REPORT</button>
        <canvas id="output_canvas" style="display:none"></canvas>
    </div>

<script type="module">
    import { FaceLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

    const themes = {
        "Neutral": { bg: "#f1f2f6", advice: "Your biometric baseline is stable. The current state suggests a balanced internal equilibrium, free from immediate environmental stressors." },
        "Thinking": { bg: "#ffeaa7", advice: "High cognitive load detected. Your neuro-spatial alignment suggests deep processing and internal data synthesis." },
        "Frustrated": { bg: "#ff7675", advice: "Elevated micro-tension detected in the brow region. This suggests a resistance to current stimuli; consider a momentary cognitive reset." },
        "Sad": { bg: "#74b9ff", advice: "Heavy aura detected. Your lowered gaze and facial vectors indicate a period of reflection or a temporary dip in emotional energy." },
        "Surprised": { bg: "#fab1a0", advice: "Sudden biometric spike. A significant shift in environmental input has momentarily breached your resting baseline." },
        "Delighted": { bg: "#55efc4", advice: "Peak flow state achieved. This organic smile indicates a high level of nervous system synchronization and positive aura resonance." }
    };

    let faceLandmarker, video = document.getElementById("webcam");
    let currentEmotion = "Neutral", lastActiveTime = Date.now(), isPaused = false;
    let baseline = {}, frameCount = 0;
    const CALIB_WINDOW = 40; 
    let timers = { Thinking: 0, Sad: 0, Frustrated: 0 };

    window.toggleModal = function() {
        const modal = document.getElementById("modal-overlay");
        isPaused = !isPaused;
        modal.style.display = isPaused ? "flex" : "none";
    }

    async function setup() {
        const filesetResolver = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm");
        faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
            baseOptions: { modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task", delegate: "GPU" },
            outputFaceBlendshapes: true, runningMode: "VIDEO", numFaces: 1
        });
        document.getElementById("loading").style.display = "none";
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } });
        video.srcObject = stream;
        video.addEventListener("loadeddata", predictWebcam);
    }

    function isDev(current, key, thresh) { return (current - (baseline[key] || 0)) > thresh; }

    async function predictWebcam() {
        if(isPaused) { window.requestAnimationFrame(predictWebcam); return; }
        const result = faceLandmarker.detectForVideo(video, performance.now());
        const now = Date.now();

        if (result.faceBlendshapes && result.faceBlendshapes.length > 0) {
            const shapes = {};
            result.faceBlendshapes[0].categories.forEach(c => shapes[c.categoryName] = c.score);
            const landmarks = result.faceLandmarks[0];

            if (frameCount < CALIB_WINDOW) {
                for (let key in shapes) baseline[key] = (baseline[key] || 0) + (shapes[key] / CALIB_WINDOW);
                frameCount++;
            } else {
                let detected = "Neutral";
                const yaw = Math.abs(landmarks[4].x - (landmarks[234].x + landmarks[454].x) / 2);
                
                if (yaw > 0.12) {
                    if (!timers.Thinking) timers.Thinking = now;
                    if (now - timers.Thinking > 2500) detected = "Thinking";
                } else {
                    timers.Thinking = 0;

                    // FRUSTRATION: Strict tension floor (0.35) to ignore normal stares
                    if (shapes.browDownLeft > 0.35 || shapes.browDownRight > 0.35) {
                        if (!timers.Frustrated) timers.Frustrated = now;
                        if (now - timers.Frustrated > 1500) detected = "Frustrated";
                    } else { timers.Frustrated = 0; }

                    // SADNESS: Inner brow rise OR sustained gaze drop
                    const innerBrowUp = isDev(shapes.browInnerUp, 'browInnerUp', 0.15);
                    const mouthFrown = shapes.mouthFrownLeft > 0.25 || shapes.mouthFrownRight > 0.25;
                    const gazeDown = (shapes.eyeLookDownLeft + shapes.eyeLookDownRight) / 2 > 0.42;

                    if (innerBrowUp || mouthFrown || gazeDown) {
                        if (!timers.Sad) timers.Sad = now;
                        if (now - timers.Sad > 2800) detected = "Sad";
                    } else { timers.Sad = 0; }

                    // JOY: Purely organic (0.45+)
                    if (detected === "Neutral") {
                        if (isDev(shapes.mouthSmileLeft, 'mouthSmileLeft', 0.45)) detected = "Delighted";
                        else if (shapes.jawOpen > 0.4 && shapes.eyeWideLeft > 0.2) detected = "Surprised";
                    }
                }

                if (detected !== "Neutral") { currentEmotion = detected; lastActiveTime = now; }
                else if (now - lastActiveTime > 2000) { currentEmotion = "Neutral"; }
            }
        }
        updateUI(currentEmotion);
        window.requestAnimationFrame(predictWebcam);
    }

    function updateUI(label) {
        const data = themes[label];
        document.body.style.background = data.bg;
        document.getElementById("emotion-label").innerText = label.toUpperCase();
        document.getElementById("advice").innerText = data.advice;
    }

    document.getElementById("btn-capture").onclick = () => {
        const canvas = document.getElementById("output_canvas");
        const ctx = canvas.getContext("2d");
        canvas.width = 640; canvas.height = 680;
        ctx.save(); ctx.scale(-1, 1); ctx.drawImage(video, -640, 0, 640, 480); ctx.restore();
        const data = themes[currentEmotion];
        ctx.fillStyle = data.bg; ctx.fillRect(0, 480, 640, 200);
        ctx.fillStyle = "#111"; ctx.font = "bold 38px sans-serif";
        ctx.fillText(currentEmotion + " AURA AUDIT", 40, 540);
        ctx.font = "18px sans-serif"; ctx.fillStyle = "#444";
        // Manual wrapping for the long advice text in capture
        const words = data.advice.split(' ');
        let line = '', y = 580;
        for(let n = 0; n < words.length; n++) {
            let testLine = line + words[n] + ' ';
            if (ctx.measureText(testLine).width > 560) {
                ctx.fillText(line, 40, y); line = words[n] + ' '; y += 25;
            } else { line = testLine; }
        }
        ctx.fillText(line, 40, y);
        const link = document.createElement('a');
        link.download = `AuraAudit_Report.png`; link.href = canvas.toDataURL(); link.click();
    };

    setup();
</script>
</body>
</html>
